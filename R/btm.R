#' @title Construct a Biterm Topic Model on Short Text
#' @description 
#' The Biterm Topic Model (BTM) is a word co-occurrence based topic model that learns topics by modeling word-word co-occurrences patterns (e.g., biterms)
#' 
#' \itemize{
#' \item A biterm consists of two words co-occurring in the same context, for example, in the same short text window. 
#' \item BTM models the biterm occurrences in a corpus (unlike LDA models which model the word occurrences in a document). 
#' \item It's a generative model. In the generation procedure, a biterm is generated by drawing two words independently from a same topic z. 
#' In other words, the distribution of a biterm \eqn{b=(wi,wj)} is defined as: \eqn{P(b) = \sum_k{P(wi|z)*P(wj|z)*P(z)}} 
#' where k is the number of topics you want to extract.
#' \item Estimation of the topic model is done with the Gibbs sampling algorithm. Where estimates are provided for \eqn{P(w|k)=phi} and \eqn{P(z)=theta}.
#' }
#' @references Xiaohui Yan, Jiafeng Guo, Yanyan Lan, Xueqi Cheng. A Biterm Topic Model For Short Text. WWW2013,
#' \url{https://github.com/xiaohuiyan/BTM}, \url{https://github.com/xiaohuiyan/xiaohuiyan.github.io/blob/master/paper/BTM-WWW13.pdf}
#' @param data a tokenised data frame containing one row per token with columns doc_id (a context identifier e.g. a tweet id, a document id, a sentence id) 
#' and token (of type character with the token)
#' @param k integer with the number of topics to identify
#' @param alpha numeric, indicating the symmetric dirichlet prior probability of a topic P(z). Defaults to 50/k.
#' @param beta numeric, indicating the symmetric dirichlet prior probability of a word given the topic P(w|z). Defaults to 0.1.
#' @param iter integer with the number of iterations of Gibbs sampling
#' @param window integer with the window size for biterm extraction. Defaults to 15.
#' @param trace logical indicating to print out evolution of the Gibbs sampling iterations. Defaults to FALSE.
#' @return an object of class BTM which is a list containing
#' \itemize{
#' \item{K: the number of topics}
#' \item{W: the number of tokens in the data}
#' \item{alpha: the symmetric dirichlet prior probability of a topic P(z)}
#' \item{beta: the symmetric dirichlet prior probability of a word given the topic P(w|z)}
#' \item{iter: the number of iterations of Gibbs sampling}
#' \item{theta: a vector with the topic probability p(z) which is determinated by the overall proportions of biterms in it}
#' \item{phi: a matrix of dimension W x K with one row for each token in the data. This matrix contains the probability of the token given the topic P(w|z).
#' the rownames of the matrix indicate the token w}
#' }
#' @export
#' @examples
#' library(udpipe)
#' data("brussels_reviews_anno", package = "udpipe")
#' x <- subset(brussels_reviews_anno, language == "nl")
#' x <- subset(x, xpos %in% c("NN", "NNP", "NNS"))
#' model  <- BTM(x, k = 5, alpha = 1, beta = 0.01, iter = 10, trace = TRUE)
#' model
#' scores <- predict(model, newdata = x)
BTM <- function(data, k = 5, alpha = 50/k, beta = 0.01, iter = 1000, window = 15, trace = FALSE){
  word <- doc_id <- NULL
  trace <- as.integer(trace)
  stopifnot(k >= 1)
  stopifnot(iter >= 1)
  stopifnot(window >= 1)
  iter <- as.integer(iter)
  window <- as.integer(window)
  stopifnot(inherits(data, "data.frame"))
  stopifnot(all(c("doc_id", "token") %in% colnames(data)))
  
  ## Convert tokens to integer numbers which need to be pasted into a string separated by spaces
  x <- data.table::setDT(data)
  x$word <- factor(x$token)
  vocabulary <- data.frame(id = seq_along(levels(x$word)) - 1L, token = levels(x$word), stringsAsFactors = FALSE)
  x$word <- as.integer(x$word) - 1L
  voc <- max(x$word) + 1
  x <- x[, list(txt = paste(word, collapse = " ")), by = list(doc_id)]
  
  ## build the model
  model <- btm(x$txt, K = k, W = voc, alpha = alpha, beta = beta, iter = iter, win = window, trace = as.integer(trace))
  
  ## make sure integer numbers are back tokens again
  rownames(model$phi) <- vocabulary$token
  class(model) <- "BTM"
  model
}

#' @export
print.BTM <- function(x, ...){
  cat("Biterm Topic Model", sep = "\n")
  cat(sprintf("  trained with %s Gibss iterations, alpha: %s, beta: %s", x$iter, x$alpha, x$beta), sep = "\n")
  cat(sprintf("  topics: %s", x$K), sep = "\n")
  cat(sprintf("  size of the token vocabulary: %s", x$W), sep = "\n")
  cat(sprintf("  topic distribution theta: %s", paste(round(x$theta, 3), collapse = " ")), sep = "\n")
}

#' @title Predict function for a Biterm Topic Model
#' @description Classify new text alongside the biterm topic model.\cr
#' 
#' To infer the topics in a document, it is assumed that the topic proportions of a document 
#' is driven by the expectation of the topic proportions of biterms generated from the document.
#' @param object an object of class BTM as returned by \code{\link{BTM}}
#' @param newdata a tokenised data frame containing one row per token with columns doc_id (a context identifier e.g. a tweet id, a document id, a sentence id) and token (of type character with the token)
#' @param type character string with the type of prediction. 
#' Either one of 'sum_b', 'sub_w' or 'mix'. Default is set to 'sum_b' as indicated in the paper, 
#' indicating to sum over the the expectation of the topic proportions of biterms generated from the document. For the other approaches, please inspect the paper.
#' @param ... not used
#' @references Xiaohui Yan, Jiafeng Guo, Yanyan Lan, Xueqi Cheng. A Biterm Topic Model For Short Text. WWW2013,
#' \url{https://github.com/xiaohuiyan/BTM}, \url{https://github.com/xiaohuiyan/xiaohuiyan.github.io/blob/master/paper/BTM-WWW13.pdf}
#' @seealso \code{\link{BTM}}
#' @return a matrix containing containing P(z|d) - the probability of the topic given the biterms.\cr
#' The matrix has one row for each unique doc_id (context identifier)
#' which contains words part of the dictionary of the BTM model and has K columns, 
#' one for each topic. 
#' @export
#' @examples 
#' library(udpipe)
#' data("brussels_reviews_anno", package = "udpipe")
#' x <- subset(brussels_reviews_anno, language == "nl")
#' x <- subset(x, xpos %in% c("NN", "NNP", "NNS"))
#' model  <- BTM(x, k = 5, iter = 5, trace = TRUE)
#' scores <- predict(model, newdata = x, type = "sum_b")
#' scores <- predict(model, newdata = x, type = "sub_w")
#' scores <- predict(model, newdata = x, type = "mix")
predict.BTM <- function(object, newdata, type = c("sum_b", "sub_w", "mix"), ...){
  word <- doc_id <- NULL
  type <- match.arg(type)
  stopifnot(inherits(newdata, "data.frame"))
  stopifnot(all(c("doc_id", "token") %in% colnames(newdata)))
  newdata <- newdata[newdata$token %in% rownames(object$phi), ]
  newdata <- data.table::setDT(newdata)
  newdata$word <- udpipe::txt_recode(newdata$token, 
                                     from = rownames(object$phi), 
                                     to = seq_along(rownames(object$phi))-1L)
  newdata <- newdata[, list(txt = paste(word, collapse = " ")), by = list(doc_id)]
  scores <- btm_infer(object, newdata$txt, type)
  rownames(scores) <- newdata$doc_id
  scores
}

#' @title Get highest token probabilities for each topic 
#' @description Get highest token probabilities for each topic 
#' @param x an object of class BTM as returned by \code{\link{BTM}}
#' @param threshold threshold in 0-1 range. Only the terms which are more likely than the threshold are returned for each topic
#' @param top_n integer indicating to return the top n tokens for each topic only 
#' @param ... not used
#' @return a list of data.frames where each data.frame contains token and probability ordered from high to low.
#' The list is the same length as the number of topics.
#' @export
#' @examples 
#' library(udpipe)
#' data("brussels_reviews_anno", package = "udpipe")
#' x <- subset(brussels_reviews_anno, language == "nl")
#' x <- subset(x, xpos %in% c("NN", "NNP", "NNS"))
#' model  <- BTM(x, k = 5, iter = 5, trace = TRUE)
#' terms(model)
#' terms(model, top_n = 10)
#' terms(model, threshold = 0.01, top_n = +Inf)
terms.BTM <- function(x, threshold = 0, top_n = 5, ...){
  apply(x$phi, MARGIN=2, FUN=function(x){
    x <- data.frame(token = names(x), probability = x)
    x <- x[x$probability >= threshold, ]
    x <- x[order(x$probability, decreasing = TRUE), ]
    rownames(x) <- NULL
    head(x, top_n)
  })
}
