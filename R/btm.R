#' @title Construct a Biterm Topic Model on Short Text
#' @description Construct a Biterm Topic Model on Short Text\cr
#' 
#' Biterm Topic Model (BTM) is a word co-occurrence based topic model that learns topics by modeling word-word co-occurrences patterns (e.g., biterms). 
#' (In constrast, LDA and PLSA are word-document co-occurrence topic models, since they model word-document co-occurrences.)\cr
#' 
#' A biterm consists of two words co-occurring in the same context, for example, in the same short text window. 
#' Unlike LDA models the word occurrences, BTM models the biterm occurrences in a corpus. 
#' In generation procedure, a biterm is generated by drawn two words independently from a same topic. 
#' In other words, the distribution of a biterm \eqn{b=(wi,wj)} is defined as:
#' \eqn{P(b) = \sum_k{P(wi|z)*P(wj|z)*P(z)}}.
#' @references Xiaohui Yan, Jiafeng Guo, Yanyan Lan, Xueqi Cheng. A Biterm Topic Model For Short Text. WWW2013,
#' \url{https://github.com/xiaohuiyan/BTM}
#' @param data a tokenised data frame containing one row per token with columns doc_id (a document identifier) and token (of type character with the token)
#' @param k integer with the number of topics to identify
#' @param alpha numeric, indicating the symmetric dirichlet prior probability of a topic P(z). Defaults to 1.
#' @param beta numeric, indicating the symmetric dirichlet prior probability of a word given the topic P(w|z). Defaults to 0.1.
#' @param iter integer with the number of iterations of Gibbs sampling
#' @param window integer with the window size for biterm extraction. Defaults to 15.
#' @param trace logical indicating to print out evolution of the Gibbs sampling iterations. Defaults to FALSE.
#' @return an object of class BTM which is a list containing
#' \itemize{
#' \item{K: the number of topics}
#' \item{W: the number of tokens in the data}
#' \item{alpha: the symmetric dirichlet prior probability of a topic P(z)}
#' \item{beta: the symmetric dirichlet prior probability of a word given the topic P(w|z)}
#' \item{iter: the number of iterations of Gibbs sampling}
#' \item{topic: a vector with the topic probability p(z) which is determinated by the overall proportions of biterms in it}
#' \item{token: a matrix of dimension W x K with one row for each token in the data. This matrix contains the probability of the token given the topic P(w|z).
#' the rownames of the matrix indicate the token w}
#' }
#' @export
#' @examples
#' library(udpipe)
#' data("brussels_reviews_anno", package = "udpipe")
#' x <- subset(brussels_reviews_anno, language == "nl")
#' x <- subset(x, xpos %in% c("NN", "NNP", "NNS"))
#' model  <- BTM(x, k = 5, alpha = 1, beta = 0.01, iter = 10, trace = TRUE)
#' scores <- predict(model, newdata = x)
BTM <- function(data, k = 5, alpha = 1, beta = 0.01, iter = 10, window = 15, trace = FALSE){
  word <- doc_id <- NULL
  trace <- as.integer(trace)
  stopifnot(k >= 1)
  stopifnot(iter >= 1)
  stopifnot(window >= 1)
  iter <- as.integer(iter)
  window <- as.integer(window)
  stopifnot(inherits(data, "data.frame"))
  stopifnot(all(c("doc_id", "token") %in% colnames(data)))
  
  ## Convert tokens to integer numbers which need to be pasted into a string separated by spaces
  x <- data.table::setDT(data)
  x$word <- factor(x$token)
  vocabulary <- data.frame(id = seq_along(levels(x$word)) - 1L, token = levels(x$word), stringsAsFactors = FALSE)
  x$word <- as.integer(x$word) - 1L
  voc <- max(x$word) + 1
  x <- x[, list(txt = paste(word, collapse = " ")), by = list(doc_id)]
  
  ## build the model
  model <- btm(x$txt, K = k, W = voc, alpha = alpha, beta = beta, iter = iter, win = window, trace = as.integer(trace))
  
  ## make sure integer numbers are back tokens again
  rownames(model$token) <- vocabulary$token
  class(model) <- "BTM"
  model
}

#' @title Predict function for a Biterm Topic Model
#' @description Classify new text alongside the biterm topic model.\cr
#' 
#' To infer the topics in a document, it is assumed that the topic proportions of a document 
#' equals to the expectation of the topic proportions of biterms generated from the document.
#' @param object an object of class BTM as returned by \code{\link{BTM}}
#' @param newdata a tokenised data frame containing one row per token with columns doc_id (a document identifier) and token (of type character with the token)
#' @param type character string with the type of prediction. 
#' Either one of 'sum_b', 'sub_w' or 'mix'. Default is set to 'sum_b' as indicated in the paper, 
#' indicating to sum over the the expectation of the topic proportions of biterms generated from the document. For the other approaches, please inspect the paper.
#' @param ... not used
#' @references Xiaohui Yan, Jiafeng Guo, Yanyan Lan, Xueqi Cheng. A Biterm Topic Model For Short Text. WWW2013,
#' \url{https://github.com/xiaohuiyan/BTM}
#' @return a matrix containing one row per document which contains words part of the dictionary of the BTM model and K columns, one for each topic. 
#' It contains P(z|d), the probability of the topic given the biterms.
#' @export
#' @examples 
#' library(udpipe)
#' data("brussels_reviews_anno", package = "udpipe")
#' x <- subset(brussels_reviews_anno, language == "nl")
#' x <- subset(x, xpos %in% c("NN", "NNP", "NNS"))
#' model  <- BTM(x, k = 5, iter = 5, trace = TRUE)
#' scores <- predict(model, newdata = x, type = "sum_b")
#' scores <- predict(model, newdata = x, type = "sub_w")
#' scores <- predict(model, newdata = x, type = "mix")
predict.BTM <- function(object, newdata, type = c("sum_b", "sub_w", "mix"), ...){
  word <- doc_id <- NULL
  type <- match.arg(type)
  stopifnot(inherits(newdata, "data.frame"))
  stopifnot(all(c("doc_id", "token") %in% colnames(newdata)))
  newdata <- newdata[newdata$token %in% rownames(object$token), ]
  newdata <- data.table::setDT(newdata)
  newdata$word <- udpipe::txt_recode(newdata$token, 
                                     from = rownames(object$token), 
                                     to = seq_along(rownames(object$token))-1L)
  newdata <- newdata[, list(txt = paste(word, collapse = " ")), by = list(doc_id)]
  scores <- btm_infer(object, newdata$txt, type)
  rownames(scores) <- newdata$doc_id
  scores
}

#' @title Get highest token probabilities for each topic 
#' @description Get highest token probabilities for each topic 
#' @param x an object of class BTM as returned by \code{\link{BTM}}
#' @param threshold threshold in 0-1 range. Only the terms which are more likely than the threshold are returned for each topic
#' @param top_n integer indicating to return the top_n
#' @param ... not used
#' @return a list of data.frames where each data.frame contains token and probability ordered from high to low.
#' The list is the same length as the number of topics.
#' @export
#' @examples 
#' library(udpipe)
#' data("brussels_reviews_anno", package = "udpipe")
#' x <- subset(brussels_reviews_anno, language == "nl")
#' x <- subset(x, xpos %in% c("NN", "NNP", "NNS"))
#' model  <- BTM(x, k = 5, iter = 5, trace = TRUE)
#' terms(model)
terms.BTM <- function(x, threshold = +Inf, top_n = 5, ...){
  apply(x$token, MARGIN=2, FUN=function(x){
    x <- data.frame(token = names(x), probability = x)
    x <- x[x$probability < threshold, ]
    x <- x[order(x$probability, decreasing = TRUE), ]
    head(x, top_n)
  })
}
