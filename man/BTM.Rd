% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/btm.R
\name{BTM}
\alias{BTM}
\title{Construct a Biterm Topic Model on Short Text}
\usage{
BTM(data, k = 5, alpha = 50/k, beta = 0.01, iter = 10, window = 15,
  trace = FALSE)
}
\arguments{
\item{data}{a tokenised data frame containing one row per token with columns doc_id (a document identifier) and token (of type character with the token)}

\item{k}{integer with the number of topics to identify}

\item{alpha}{numeric, indicating the symmetric dirichlet prior probability of a topic P(z). Defaults to 50/k.}

\item{beta}{numeric, indicating the symmetric dirichlet prior probability of a word given the topic P(w|z). Defaults to 0.1.}

\item{iter}{integer with the number of iterations of Gibbs sampling}

\item{window}{integer with the window size for biterm extraction. Defaults to 15.}

\item{trace}{logical indicating to print out evolution of the Gibbs sampling iterations. Defaults to FALSE.}
}
\value{
an object of class BTM which is a list containing
\itemize{
\item{K: the number of topics}
\item{W: the number of tokens in the data}
\item{alpha: the symmetric dirichlet prior probability of a topic P(z)}
\item{beta: the symmetric dirichlet prior probability of a word given the topic P(w|z)}
\item{iter: the number of iterations of Gibbs sampling}
\item{theta: a vector with the topic probability p(z) which is determinated by the overall proportions of biterms in it}
\item{phi: a matrix of dimension W x K with one row for each token in the data. This matrix contains the probability of the token given the topic P(w|z).
the rownames of the matrix indicate the token w}
}
}
\description{
Construct a Biterm Topic Model on Short Text\cr

Biterm Topic Model (BTM) is a word co-occurrence based topic model that learns topics by modeling word-word co-occurrences patterns (e.g., biterms). 
(In constrast, LDA and PLSA are word-document co-occurrence topic models, since they model word-document co-occurrences.)\cr

A biterm consists of two words co-occurring in the same context, for example, in the same short text window. 
Unlike LDA models the word occurrences, BTM models the biterm occurrences in a corpus. 
In generation procedure, a biterm is generated by drawn two words independently from a same topic. 
In other words, the distribution of a biterm \eqn{b=(wi,wj)} is defined as:
\eqn{P(b) = \sum_k{P(wi|z)*P(wj|z)*P(z)}}.
}
\examples{
library(udpipe)
data("brussels_reviews_anno", package = "udpipe")
x <- subset(brussels_reviews_anno, language == "nl")
x <- subset(x, xpos \%in\% c("NN", "NNP", "NNS"))
model  <- BTM(x, k = 5, alpha = 1, beta = 0.01, iter = 10, trace = TRUE)
model
scores <- predict(model, newdata = x)
}
\references{
Xiaohui Yan, Jiafeng Guo, Yanyan Lan, Xueqi Cheng. A Biterm Topic Model For Short Text. WWW2013,
\url{https://github.com/xiaohuiyan/BTM}
}
